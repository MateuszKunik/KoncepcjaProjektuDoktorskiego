# KoncepcjaProjektuDoktorskiego
Koncepcja projektu doktorskiego zatytułowana: Optymalizacja czasu procesu inferencji modeli bazowych na przykładzie modeli dyfuzyjnych


**Mateusz Kunik **

** Optymalizacja czasu procesu inferencji modeli bazowych na przykładzie modeli dyfuzyjnych**

*Streszczenie:*

Projekt doktorski koncentruje się na opracowaniu metod optymalizacji czasu inferencji w modelach generatywnych, ze szczególnym uwzględnieniem modeli dyfuzyjnych. Modele te zyskały uznanie dzięki zdolności do generowania realistycznych obrazów, takich jak twarze, postacie i krajobrazy. Głównym celem projektu jest identyfikacja i rozwiązanie problemów związanych z długim czasem inferencji, dużym rozmiarem modeli i kosztownymi obliczeniami. Projekt ma na celu poprawę wydajności modeli dyfuzyjnych oraz poszerzenie ich zastosowań w dziedzinie przetwarzania obrazów i sztucznej inteligencji.

*Zakres tematyczny:*

W ostatnich latach modele generatywne zyskują na popularności w dziedzinie przetwarzania obrazów, dźwięku i tekstu. Modele dyfuzyjne są jednymi z najbardziej obiecujących modeli generatywnych, ze względu na ich zdolność do generowania wysokiej jakości obrazów oraz odzwierciedlanie złożonych wzorców i tekstur. Jednak mają one pewne wady, takie jak długi czas inferencji, duży rozmiar i wysokie zapotrzebowanie na moc obliczeniową. Projekt doktorski skupi się na badaniach mających na celu skrócenie czasu inferencji, redukcję rozmiaru modeli oraz obniżenie kosztów obliczeniowych.

*Wstępny plan badań:*

Projekt zostanie zrealizowany w kilku etapach:
1. **Przegląd literatury:** Analiza istniejących prac naukowych z zakresu głębokich modeli generatywnych, modeli dyfuzyjnych oraz technik optymalizacji inferencji.
2. **Analiza i identyfikacja wyzwań:** Dokładna analiza procesu inferencji modeli dyfuzyjnych w celu zidentyfikowania głównych problemów i ograniczeń.
3. **Opracowanie metod optymalizacji:** Propozycja nowych technik optymalizacji czasu inferencji, które poprawią wydajność modeli dyfuzyjnych.
4. **Implementacja eksperymentów:** Stworzenie środowiska eksperymentalnego do przeprowadzenia testów i eksperymentów oceniających proponowane techniki optymalizacji.
5. **Ewaluacja eksperymentalna:** Ocena wydajności proponowanych rozwiązań na podstawie ustalonych metryk.
6. **Analiza i ocena wyników:** Porównanie różnych strategii optymalizacji oraz wyciągnięcie wniosków na temat ich efektywności.

*Uzasadnienie wyboru tematu:*

Projekt doktorski ma na celu rozwiązanie istotnych problemów związanych z czasem inferencji w modelach dyfuzyjnych. Skrócenie tego czasu jest kluczowe dla zastosowań w czasie rzeczywistym oraz redukcji kosztów obliczeniowych. Wdrażając kwantyzację i minimalizację rozmiaru modelu, projekt może przyczynić się do bardziej efektywnego wykorzystania modeli dyfuzyjnych w praktyce, co ma znaczenie w dziedzinach takich jak przetwarzanie obrazów i sztuczna inteligencja.

*Literatura:*

[1] Jonathan Ho, Ajay Jain i Pieter Abbeel. Denoising Diffusion Probabilistic Models, arXiv:2006.11239, 2020.

[2] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan i Surya Ganguli. Deep Unsupervised Learning using Nonequilibrium Thermodynamics, arXiv:1503.03585, 2015.

[3] Yang Song i Stefano Ermon. Improved Techniques for Training Score-Based Generative Models, arXiv:2006.09011, 2020.

[4] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao i Bryan Catanzaro. DiffWave: A Versatile Diffusion Model for Audio Synthesis, arXiv:2009.09761, 2021.

[5] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi i William Chan. WaveGrad: Estimating Gradients for Waveform Generation, arXiv:2009.00713, 2020.

[6] Prafulla Dhariwal i Alex Nichol. Diffusion Models Beat GANs on Image Synthesis, arXiv:2105.05233, 2021.

[7] Alex Nichol i Prafulla Dhariwal. Improved Denoising Diffusion Probabilistic Models, arXiv:2102.09672, 2021.

[8] Jiaming Song, Chenlin Meng i Stefano Ermon. Denoising Diffusion Implicit Models, arXiv:2010.02502, 2020.

[9] Tim Salimans i Jonathan Ho. Progressive Distillation for Fast Sampling of Diffusion Models, arXiv:2202.000512, 2022.

[10] Lvmin Zhang i Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models, arXiv:2302.05543, 2023.

[11] Zhisheng Xiao, Karsten Kreis i Arash Vahdat. Tackling the Generative Learning Trilemma with Denoising Diffusion GANs, arXiv:2112.07804, 2021.

[12] Yang Song, Prafulla Dhariwal, Mark Chen i Ilya Sutskever. Consistency Models, arXiv:2303.01469, 2023.
